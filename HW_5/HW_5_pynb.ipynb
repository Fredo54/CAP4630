{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.pynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkOJpOGqqSp/0sC2LdIgyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fredo54/CAP4630/blob/master/HW_5/HW_5_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdh0gElIBOgt",
        "colab_type": "text"
      },
      "source": [
        "# General concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiWQu4bMBRko",
        "colab_type": "text"
      },
      "source": [
        "What is Artificial Intelligence?\n",
        "\n",
        "Artificial intelligence is very broad with multiple subsets like machine learning and deep learning. Machine Learning and Deep Learning are within AI but AI isn't solely Machine Learning and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzH05R_6Bu4I",
        "colab_type": "text"
      },
      "source": [
        "What is Machine Learning?\n",
        "\n",
        "The machine learning process is for a model to modify itself when it is exposed to data. Machine learning does not need any users to run after a model is trained. This is done as the model \"learns\", hence the name Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ydWAjWCtCa",
        "colab_type": "text"
      },
      "source": [
        "What is Deep Learning?\n",
        "\n",
        "Deep Learning specifies that there are layers in neural networks. Neural networks have many hidden layers that allow them to learn many features of the data without user's supervision. The data structure used are called artifical neural networks. Artificial Neural Networks are done using directed weighted graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qpd6QB6FqIz",
        "colab_type": "text"
      },
      "source": [
        "# Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6cxWB01F0n-",
        "colab_type": "text"
      },
      "source": [
        "What is Linear Regression?\n",
        "\n",
        "What is so beautiful about linear regression, is that it is simply just y = mx + b. The objective of linear regression is to approximate the equation of a line based on the input data and label values specified. \n",
        "Linear regression with multiple weights can be represented as \n",
        "\n",
        "y = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJnJYQ83HrJw",
        "colab_type": "text"
      },
      "source": [
        "What is Logistic Regression?\n",
        "\n",
        "We use logistic regression for binary classificatoin problems. Classifying based on whether the subject is A or B. Logistic regression uses a sigmoid function hat activates within the interval [0, 1] and based on the value being closer to 0 or 1 it will round or truncate to that number.\n",
        "Sigmoid Function : $\\sigma(x)$ = $\\frac{1}{1+e^{-x}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfaBgTzD3se",
        "colab_type": "text"
      },
      "source": [
        "What are Gradients?\n",
        "\n",
        "The gradient is a vector where each element in the vector is a partial derivative of a multivariable function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv4q3bLvEqI8",
        "colab_type": "text"
      },
      "source": [
        "What is Gradient Descent?\n",
        "\n",
        "Gradient descent is an implementation of calculating slope to minimize loss.\n",
        "Calculus allows us to determine the slope throughout the multivariable function. With Machine Learning we take the slope of the loss function with respect to all the weights and try to minimize the total loss of the model. \n",
        "By taking the gradient with respect to each weight we can adjust the weights for each directoin and minimize the total loss which improves the model.\n",
        "The different types of gradient descent we have done are stochastic, batch, and mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q56WDSYbEqbb",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y12jdIyGIEV",
        "colab_type": "text"
      },
      "source": [
        "What is the structure of a Convent?\n",
        "\n",
        "Convoltions are operations that involve two inputs. An input matrix, and a kernel or filter are passed. In convolutions the kernel iterates through the columns and rows of the input matrix by a specified length. If the input, kernel, and stride length don't match then the input matrix can be padded with zeros to perform the operation. The kernel iterates and considers each subset of the input matrix. The dot product is done with teh subset of the input matrix and the kernel, then all values are added together with the final value becoming the output matrix.\n",
        "\n",
        "\n",
        "Max pooling is an operation in which an input matrix and a window size of n by n is specified. The windows iterates throught the input matrix and considers a non overlapping region of n by n. The maximum value in that region becomes one of the values in the output matrix. Padding is allowed if input matrix and window size do not match.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA6uBNGtJsCW",
        "colab_type": "code",
        "outputId": "c9b78e03-2995-46e8-a6e0-c264781265f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class invalidInput(Exception):\n",
        "  print(\"Error: Input matrix is invalid!\")\n",
        "  exit(1)\n",
        "  \n",
        "class invalidKernel(Exception):\n",
        "  print(\"Error: Kernel matrix is invalid!\")\n",
        "  exit(1)\n",
        "\n",
        "  \n",
        "def conv2d(input_mat, kernel_mat):\n",
        "  #input_mat = np.matrix(input_mat)\n",
        "  #kernel_mat = np.matrix(kernel_mat)\n",
        "  # check for square inputs\n",
        "  if kernel_mat.shape[0] != kernel_mat.shape[1]:\n",
        "    raise invalidKernel()\n",
        "  elif input_mat.shape[0] != input_mat.shape[1]:\n",
        "    raise invalidInput()\n",
        "  else:\n",
        "    # get the dimensions of the output layer\n",
        "    n = input_mat.shape[0] - kernel_mat.shape[0] + 1\n",
        "    m = kernel_mat.shape[0]\n",
        "  \n",
        "\n",
        "  output_mat = np.zeros((n, n))\n",
        "\n",
        "  for i in range(n):\n",
        "    for k in range(n):\n",
        "      # multiply by kernel filter\n",
        "      #print(np.multiply(input_mat[i:i+m, k:k+m], kernel_mat))\n",
        "      #print()\n",
        "      conv = np.multiply(input_mat[i:i+m, k:k+m], kernel_mat)\n",
        "      # sum all elements\n",
        "      output_mat[i,k] = np.sum(conv)\n",
        "\n",
        "  return output_mat\n",
        "\n",
        "input_mat = np.array([[1, 2, 1, 2],\n",
        "                     [2, 1, 2, 1],\n",
        "                     [1, 2, 1, 2],\n",
        "                     [2, 1, 2, 1]])\n",
        "\n",
        "kernel_mat = np.array([[1, 0],\n",
        "                      [0, 1]])\n",
        "\n",
        "print(conv2d(input_mat, kernel_mat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: Input matrix is invalid!\n",
            "Error: Kernel matrix is invalid!\n",
            "[[2. 4. 2.]\n",
            " [4. 2. 4.]\n",
            " [2. 4. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SehEIdlKZDv",
        "colab_type": "text"
      },
      "source": [
        "# Comping a Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB-6Ow7FKfal",
        "colab_type": "text"
      },
      "source": [
        "Compiling a model with keras can be done by instantiating a Model and configuring its necessary layers. There are several parameters that should be specified when compiling the model, such as which optimizer instance is needed for training the model.\n",
        "\n",
        "\n",
        "Loss Function (objective function)\n",
        "A loss function generates the error of a single training example, and is typically used for minimizing cost with an optimization method such as gradient descent; It is also used as a measure of success for the current task.\n",
        "\n",
        "The loss functions learned about in this course are:\n",
        "\n",
        "Squared Error Loss: The square difference between actual and predicted values\n",
        "\n",
        "$L = (y - f(x))^2$\n",
        "\n",
        "Binary Cross Entropy Loss: also named Sigmoid Cross-Entropy loss\n",
        "\n",
        "$L = (Y)(-log(Y_{pred})) + (1-Y)(-log(1-Y_{pred}) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orWkSJiwVAFq",
        "colab_type": "text"
      },
      "source": [
        "Learning Rate is typically applied to a gradient to allow for smaller steps toward a minimum of the loss function. If the Learning rate is too small the model can take too long to train or get stuck at a local minimum. If the learning rate is too large the model may overshoot the minimum of the loss function and have a diverging loss. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvDSxusNUPoP",
        "colab_type": "text"
      },
      "source": [
        "Optimizers are used to help the Gradient Descent. They apply different strategies to get faster and more accurate training results.\n",
        "\n",
        "RMSProp and SGD are popular Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMUPS5myUgqh",
        "colab_type": "text"
      },
      "source": [
        "# Training a Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQPjZ5UAUrIc",
        "colab_type": "text"
      },
      "source": [
        "Overfitting is when your model performs exceptionally well to your training data but not on the testing data or any new information. This is usually because the model accepts irrevelant information as part of it's algorithm or the \"noise\" of the data. If the data fits the model too well with low bias and high variance, new data will not be categorized completely.\n",
        "\n",
        "The fixes for this may be both removing features or training with more data. In the off chance where you capture a specific set of data, more samples may normalize your data. Removing features that are less important will remove more noise from a redundant model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjgeZ6DSUyoR",
        "colab_type": "text"
      },
      "source": [
        "Underfitting normally results when the model has high loss for both the validation and training data. This can be resolved by either a different learning rate or a more complicated model. It may also be due to the model not being trained long enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD76LcreWlWC",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning a pretrained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qERHRCWcXnKq",
        "colab_type": "text"
      },
      "source": [
        "Fine tuning is when we take an already trained model and use it to build a new model on a different data set. Companies like Google have trained models that are available to the public. We can download these models and use them and modify them with our own data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWqKmbiAXNot",
        "colab_type": "text"
      },
      "source": [
        "When finetuning a pretrained model there are some parts of the model we don't want to affect as they are accurate as is. It is best to freeze many layers that are accurate in a pretrained model like the upper layers. By freezing those layers we can train the bottom layers to get better results on our model."
      ]
    }
  ]
}